{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c40d7a-2ddc-4ef2-9ae7-5fbf1b896f1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Orginsation of EDM tracks with Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76399e2b-8dbf-473b-a252-9e175ee87c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import csv\n",
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import seaborn as sns\n",
    "import time\n",
    "import copy\n",
    "import pyrubberband as pyrb\n",
    "from ipynb.fs.full.musicalkeyfinder import *\n",
    "from matplotlib.lines import Line2D\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "sns.set(style=\"darkgrid\", palette=\"muted\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"axes.labelsize\"] = 13\n",
    "plt.rcParams[\"axes.titlesize\"] = 13\n",
    "plt.rcParams[\"figure.figsize\"] = 16, 10\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"xtick.labelsize\"] = 11\n",
    "plt.rcParams[\"ytick.labelsize\"] = 11\n",
    "plt.rcParams[\"legend.fontsize\"] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a437fb51-4884-4f0a-9e2b-29d1d7b1bcf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Internal utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4f814-a901-4f06-907c-fc530ef2e36e",
   "metadata": {},
   "source": [
    "### Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb1ae0-8a1b-458a-9fe8-933588215023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extractTrackDataAndWriteToCSV(y, sr, output_csv):\n",
    "\n",
    "    #----------------------------------------------------------------------------\n",
    "    #-- Preperations --#\n",
    "    hop_length = 512\n",
    "    \n",
    "    #seperate harmonics from percussives\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    # Musical-key-extractor by jackmcarthur (git-project)\n",
    "    keyextract = Tonal_Fragment(y_harmonic, sr=sr)   \n",
    "    # Separate a complex-valued spectrogram into its magnitude and phase\n",
    "    S, phase = librosa.magphase(librosa.stft(y))\n",
    "    # Compute onset strengths for tempo estimation purposes\n",
    "    oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    #----------------------------------------------------------------------------\n",
    "    #-- Extraction --#    \n",
    "    \n",
    "    # Extract tempo - beats per minute\n",
    "    tempo = float(librosa.beat.tempo(onset_envelope=oenv, sr=sr))\n",
    "    # Extract onsets and devide by length to get rate\n",
    "    onset = librosa.onset.onset_detect(y=y, sr=sr)\n",
    "    onset_rate = (np.sum(onset) / np.size(onset))\n",
    "    # Extract zero crossings rate\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=y))\n",
    "    # Extract spectral centroid\n",
    "    spec_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "    # Extract spectral contrast\n",
    "    spec_contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr)) \n",
    "    # Extract spectral rolloff\n",
    "    spec_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))   \n",
    "    # Extract spectral_bandwidth -\n",
    "    spec_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))  \n",
    "    # Extract tonnetz - \n",
    "    tonnetz = librosa.feature.tonnetz(y=y, sr=sr, chroma=None)\n",
    "    # Extract MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    #----------------------------------------------------------------------------\n",
    "    #-- Processing --# \n",
    "    \n",
    "    # Appends mean values of all arrays of data\n",
    "    to_append = f'{tempo:5.3f} {onset_rate:5.3f} {zcr:5.3f} {spec_centroid:5.3f} {spec_contrast:5.3f} {spec_rolloff:5.3f} {spec_bandwidth:5.3f}'\n",
    "    \n",
    "    # Add tonnetz\n",
    "    for t in tonnetz:\n",
    "        to_append += f' {np.sum(t):5.3f}'\n",
    "        \n",
    "    # Add chroma tones\n",
    "    keyextract.chroma_max = max(keyextract.chroma_vals)\n",
    "    for key, chrom in keyextract.keyfreqs.items():\n",
    "        to_append += f' {(chrom / keyextract.chroma_max):5.3f}'\n",
    "        \n",
    "    # Add mfccs\n",
    "    for i in range(1, 13): #for e in mfcc:\n",
    "        to_append += f' {np.mean(mfcc[i]):5.3f}' #f' {np.mean(e):5.3f}'\n",
    "        \n",
    "    # Split string into a list and insert track name\n",
    "    arguments = to_append.split()\n",
    "    arguments.insert(0, track)\n",
    "    \n",
    "    # Write to data.csv\n",
    "    file = open(output_csv, 'a', newline='')\n",
    "    with file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625b6c0-a04c-4727-8490-e4a0c4c3b77d",
   "metadata": {},
   "source": [
    "### Alternative feature extractor with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339975e-818a-464c-b553-f19f29c986ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTrackDataAndWriteToCSV_2(y, sr, output_csv):\n",
    "\n",
    "    #----------------------------------------------------------------------------\n",
    "    #-- Preperations --#\n",
    "    hop_length = 512\n",
    "    \n",
    "    #seperate harmonics from percussives\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    # Musical-key-extractor by jackmcarthur (git-project)\n",
    "    keyextract = Tonal_Fragment(y_harmonic, sr=sr)   \n",
    "    # Separate a complex-valued spectrogram into its magnitude and phase\n",
    "    S, phase = librosa.magphase(librosa.stft(y))\n",
    "    # Compute onset strengths for tempo estimation purposes\n",
    "    oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    #----------------------------------------------------------------------------\n",
    "    #-- Extraction --#    \n",
    "    \n",
    "    # Extract tempo - beats per minute\n",
    "    tempo = float(librosa.beat.tempo(onset_envelope=oenv, sr=sr))\n",
    "    \n",
    "    # Extract onsets and devide by length to get rate\n",
    "    onset = librosa.onset.onset_detect(y=y, sr=sr)\n",
    "    onset_rate = (np.sum(onset) / np.size(onset))\n",
    "    \n",
    "    # Extract onset strength\n",
    "    res = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    onset_strength_mean = np.mean(res)\n",
    "    onset_strength_std = np.std(res)\n",
    "    \n",
    "    # Extract zero crossings rate\n",
    "    res = librosa.feature.zero_crossing_rate(y=y)\n",
    "    zcr_mean = np.mean(res)\n",
    "    zcr_std = np.std(res)\n",
    "    \n",
    "    # Extract spectral centroid\n",
    "    res = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_centroid_mean = np.mean(res)\n",
    "    spec_centroid_std = np.std(res)\n",
    "    \n",
    "    # Extract spectral contrast\n",
    "    res = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    spec_contrast_mean = np.mean(res) \n",
    "    spec_contrast_std = np.std(res) \n",
    "    \n",
    "    # Extract spectral rolloff\n",
    "    res = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    spec_rolloff_mean = np.mean(res)  \n",
    "    spec_rolloff_std = np.std(res) \n",
    "    \n",
    "    # Extract spectral_bandwidth -\n",
    "    res = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spec_bandwidth_mean = np.mean(res)  \n",
    "    spec_bandwidth_std = np.std(res)  \n",
    "    \n",
    "    # Extract spectral_flatness -\n",
    "    res = librosa.feature.spectral_flatness(y=y)\n",
    "    spectral_flatness_mean = np.mean(res)\n",
    "    spectral_flatness_std = np.std(res)\n",
    "    \n",
    "    # Extract tonnetz - \n",
    "    tonnetz = librosa.feature.tonnetz(y=y, sr=sr, chroma=None)\n",
    "    \n",
    "    # Extract MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    #----------------------------------------------------------------------------\n",
    "    #-- Processing --# \n",
    "    \n",
    "    # Appends mean values of all arrays of data\n",
    "    to_append = f'{tempo:5.3f} {onset_rate:5.3f}'\n",
    "    \n",
    "    to_append += f' {onset_strength_mean:5.3f} {onset_strength_std:5.3f}'\n",
    "    to_append += f' {zcr_mean:5.3f} {zcr_std:5.3f}'\n",
    "    to_append += f' {spec_centroid_mean:5.3f} {spec_centroid_std:5.3f}'\n",
    "    to_append += f' {spec_contrast_mean:5.3f} {spec_contrast_std:5.3f}'\n",
    "    to_append += f' {spec_rolloff_mean:5.3f} {spec_rolloff_std:5.3f}'\n",
    "    to_append += f' {spec_bandwidth_mean:5.3f} {spec_bandwidth_std:5.3f}'\n",
    "    to_append += f' {spectral_flatness_mean:8.6f} {spectral_flatness_std:8.6f}'\n",
    "    \n",
    "    # Add tonnetz\n",
    "    for t in tonnetz:\n",
    "        to_append += f' {np.mean(t):5.3f}'\n",
    "        to_append += f' {np.std(t):5.3f}'\n",
    "        \n",
    "    # Add chroma tones\n",
    "    keyextract.chroma_max = max(keyextract.chroma_vals)\n",
    "    for key, chrom in keyextract.keyfreqs.items():\n",
    "        to_append += f' {(chrom / keyextract.chroma_max):5.3f}'\n",
    "        \n",
    "    # Add mfccs\n",
    "    for i in range(1, 13): #for e in mfcc:\n",
    "        to_append += f' {np.mean(mfcc[i]):5.3f}' #f' {np.mean(e):5.3f}'\n",
    "        to_append += f' {np.std(mfcc[i]):5.3f}' #f' {np.std(e):5.3f}'\n",
    "        \n",
    "    # Split string into a list and insert track name\n",
    "    arguments = to_append.split()\n",
    "    arguments.insert(0, track)\n",
    "    \n",
    "    # Write to data.csv\n",
    "    file = open(output_csv, 'a', newline='')\n",
    "    with file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee9f40-e895-4cfd-9e0c-97a1cdd11803",
   "metadata": {},
   "source": [
    "### Plot utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6feb7d-119f-4c45-bc6c-c085838d6e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fig_ax(figsize=(15, 5), dpi=150):\n",
    "    \"\"\"Return a (matplotlib) figure and ax objects with given size.\"\"\"\n",
    "    return plt.subplots(figsize=figsize, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af959ddc-6e0a-48a4-9cf7-afa50ae90eab",
   "metadata": {},
   "source": [
    "### Laplacian Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1db228-3fe2-4dc5-a7ca-7f77858f6ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code source: Brian McFee\n",
    "# License: ISC\n",
    "# Eited by me as part of my master thesis: Victor Tideman\n",
    "\n",
    "# implements the laplacian segmentation method of `McFee and Ellis, \n",
    "# 2014 <http://bmcfee.github.io/papers/ismir2014_spectral.pdf>`_, \n",
    "# with a couple of minor stability improvements.\n",
    "\n",
    "def laplacian(y, sr, nr_klusters=6, track_name=\"\"):\n",
    "    \n",
    "    # Next, we'll compute and plot a log-power CQT\n",
    "    BINS_PER_OCTAVE = 12 * 3\n",
    "    N_OCTAVES = 7\n",
    "    C = librosa.amplitude_to_db(np.abs(librosa.cqt(y=y, sr=sr, bins_per_octave=BINS_PER_OCTAVE, n_bins=N_OCTAVES * BINS_PER_OCTAVE)), ref=np.max)\n",
    "\n",
    "    # To reduce dimensionality, we'll beat-synchronous the CQT\n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr, trim=False)\n",
    "    Csync = librosa.util.sync(C, beats, aggregate=np.median)\n",
    "\n",
    "    # For plotting purposes, we'll need the timing of the beats\n",
    "    # we fix_frames to include non-beat frames 0 and C.shape[1] (final frame)\n",
    "    beat_times = librosa.frames_to_time(librosa.util.fix_frames(beats, x_min=0), sr=sr)\n",
    "\n",
    "    # Let's build a weighted recurrence matrix using beat-synchronous CQT(Equation 1)\n",
    "    # width=3 prevents links within the same bar\n",
    "    # mode='affinity' here implements S_rep (after Eq. 8)\n",
    "    R = librosa.segment.recurrence_matrix(Csync, width=3, mode='affinity', sym=True)\n",
    "\n",
    "    # Enhance diagonals with a median filter (Equation 2)\n",
    "    df = librosa.segment.timelag_filter(scipy.ndimage.median_filter)\n",
    "    Rf = df(R, size=(1, 7))\n",
    "    \n",
    "    # Now let's build the sequence matrix (S_loc) using mfcc-similarity\n",
    "    # sigma to be the median distance between successive beats.\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    Msync = librosa.util.sync(mfcc, beats)\n",
    "    path_distance = np.sum(np.diff(Msync, axis=1)**2, axis=0)\n",
    "    sigma = np.median(path_distance)\n",
    "    path_sim = np.exp(-path_distance / sigma)\n",
    "    R_path = np.diag(path_sim, k=1) + np.diag(path_sim, k=-1)\n",
    "    \n",
    "    # And compute the balanced combination (Equations 6, 7, 9)\n",
    "    deg_path = np.sum(R_path, axis=1)\n",
    "    deg_rec = np.sum(Rf, axis=1)\n",
    "    mu = deg_path.dot(deg_path + deg_rec) / np.sum((deg_path + deg_rec)**2)\n",
    "    A = mu * Rf + (1 - mu) * R_path\n",
    "    \n",
    "    # Now let's compute the normalized Laplacian (Eq. 10)\n",
    "    L = scipy.sparse.csgraph.laplacian(A, normed=True)\n",
    "\n",
    "    # and its spectral decomposition\n",
    "    evals, evecs = scipy.linalg.eigh(L)\n",
    "    \n",
    "    # Apply a median filter to help smooth over small discontinuities and clean up.\n",
    "    evecs = scipy.ndimage.median_filter(evecs, size=(9, 1))\n",
    "\n",
    "    # cumulative normalization is needed for symmetric normalize laplacian eigenvectors\n",
    "    Cnorm = np.cumsum(evecs**2, axis=1)**0.5\n",
    "\n",
    "    # Use the first k normalized eigenvectors.\n",
    "    k = nr_klusters\n",
    "    X = evecs[:, :k] / Cnorm[:, k-1:k]\n",
    "\n",
    "    # Let's use these k components to cluster beats into segments (Algorithm 1)\n",
    "    KM = sklearn.cluster.KMeans(n_clusters=k)\n",
    "    seg_ids = KM.fit_predict(X)\n",
    "    colors = plt.get_cmap('Paired', k)\n",
    "\n",
    "    # Locate segment boundaries from the label sequence\n",
    "    bound_beats = 1 + np.flatnonzero(seg_ids[:-1] != seg_ids[1:])\n",
    "    # Count beat 0 as a boundary\n",
    "    bound_beats = librosa.util.fix_frames(bound_beats, x_min=0)\n",
    "    # To avoid array out of index issue in some cases <- quick fix\n",
    "    bound_beats = bound_beats[:-1]\n",
    "    # Compute the segment label for each boundary\n",
    "    bound_segs = list(seg_ids[bound_beats])\n",
    "    # Convert beat indices to frames\n",
    "    bound_frames = beats[bound_beats]\n",
    "    # Make sure we cover to the end of the track\n",
    "    bound_frames = librosa.util.fix_frames(bound_frames, x_min=None, x_max=C.shape[1]-1)\n",
    "    # Convert frames to time\n",
    "    bound_times = librosa.frames_to_time(bound_frames)\n",
    "    \n",
    "    # And plot the final segmentation over original CQT\n",
    "    #freqs = librosa.cqt_frequencies(n_bins=C.shape[0], fmin=librosa.note_to_hz('C1'), bins_per_octave=BINS_PER_OCTAVE)\n",
    "    #fig, ax = plt.subplots()\n",
    "    #librosa.display.specshow(C, y_axis='cqt_hz', sr=sr, bins_per_octave=BINS_PER_OCTAVE, x_axis='time', ax=ax)\n",
    "\n",
    "    #for interval, label in zip(zip(bound_times, bound_times[1:]), bound_segs):\n",
    "    #    ax.add_patch(patches.Rectangle((interval[0], freqs[0]), interval[1] - interval[0], freqs[-1], facecolor=colors(label), alpha=0.50))\n",
    "        \n",
    "    return bound_times, bound_segs, nr_klusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c99df4-762b-474d-b7f7-e58696b8bad3",
   "metadata": {},
   "source": [
    "### Extraction point selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf2dbb-6bc7-401d-a23a-547c22b56491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractStartingPoint(y, sr, trackpath):\n",
    "    \n",
    "    # Compute segments/timestamps where change is detected using laplacian method\n",
    "    bound_times, bound_segs, nr_klusters = laplacian(y=y, sr=sr, nr_klusters=6, track_name=track)\n",
    "    \n",
    "    # Hacky fix to issue of some breakpoints being after the track\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    original_size = bound_times.size\n",
    "    bound_times = bound_times[(bound_times < duration)]\n",
    "    new_size = bound_times.size\n",
    "    nr_remove = original_size - new_size\n",
    "    bound_segs = bound_segs[:-nr_remove]\n",
    "    \n",
    "    # Initialise\n",
    "    selected_breakpoint = None\n",
    "\n",
    "    # Get number of breakpoints (one less segments in total, segments exist between breakpoints)\n",
    "    nr_breakpoints = bound_times.size\n",
    "    \n",
    "    # For tracing: print potential breakpoints\n",
    "    #print(f'Potential breakpoints: {bound_times}')\n",
    "\n",
    "    # Ignore intro and outro (they can be in the same cluster)\n",
    "    intro_idx = bound_segs[0]\n",
    "    # outro_idx = bound_segs[-1]\n",
    "\n",
    "    # Create list for storing potential segments\n",
    "    potential_segments = []\n",
    "\n",
    "    # Add every segment that is not too short or of the same index as intro_idx or outro_idx\n",
    "    # to a list of potential segments to use\n",
    "    for i in range(0, nr_breakpoints - 1):\n",
    "        seg_id = bound_segs[i]\n",
    "        if (seg_id != intro_idx):\n",
    "\n",
    "            # Get duration of a segment\n",
    "            segment_duration = bound_times[i+1] - bound_times[i]\n",
    "\n",
    "            # Make sure segment is longer than minimum duration\n",
    "            if (segment_duration > float(min_duration)):\n",
    "                potential_segments.append(bound_times[i])\n",
    "\n",
    "    # make a numpy array\n",
    "    segment_startpoints = np.array(potential_segments)\n",
    "    nr_seg = segment_startpoints.size\n",
    "\n",
    "    # Unless only 1 segment remains, compare remaining segments to select the most \"expressable\" segment\n",
    "    if (nr_seg > 1):\n",
    "        score_list = []\n",
    "\n",
    "        #print(segment_startpoints)\n",
    "        \n",
    "        # Loop through the promising segments\n",
    "        for segment_start in segment_startpoints:\n",
    "            # We want a segment that is active in all frequency bands (base, mid and percussives)\n",
    "            #print(f'load_segment at: {segment_start}')\n",
    "            y_seg, sr = librosa.load(trackpath, mono=True, sr=44100, offset=segment_start, duration=extract_window_duration)\n",
    "            S, phase = librosa.magphase(librosa.stft(y_seg))\n",
    "            rms = np.mean(librosa.feature.rms(S=S))\n",
    "\n",
    "            # Add segment value to the array\n",
    "            score_list.append(rms)\n",
    "\n",
    "        # Pick the segment with the largest value in the array\n",
    "        selected_breakpoint = segment_startpoints[np.argmax(np.array(score_list))]\n",
    "\n",
    "    # One or less viable breakpoints\n",
    "    else:\n",
    "        if (segment_startpoints.size != 0):\n",
    "            selected_breakpoint = segment_startpoints[0]\n",
    "        else:\n",
    "            selected_breakpoint = 0\n",
    "            \n",
    "    return selected_breakpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac767392-5063-4e30-a675-4225a4ffb9e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract features from tracks into a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046d77a-5ccf-41f6-b70f-b497340d9333",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Header for CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609908e3-ce9e-40f5-9225-f1babbb13625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create headers\n",
    "header = 'track_name bpm onset_rate zcr spctrl_cent spctrl_cont spctrl_roll spctrl_band' \n",
    "\n",
    "# Add tonnetz to header\n",
    "for i in range(1, 7):\n",
    "    header += f' tonnetz_mean{i}'\n",
    "    \n",
    "# Add tones to header\n",
    "pitches = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n",
    "for i in range(0, 12):\n",
    "    header += f' {pitches[i]}'\n",
    "\n",
    "# Add mfccs to header\n",
    "for i in range(1, 13):\n",
    "    header += f' mfcc_mean{i+1}'\n",
    "    \n",
    "header = header.split()\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ea0a4-d8c8-45d8-b3d7-11b98fa925bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_large = 'track_name bpm onset_rate' \n",
    "header_large += ' onset_strength_mean onset_strength_std'\n",
    "header_large += ' zcr_mean zcr_std'\n",
    "header_large += ' spec_cent_mean spec_cent_std'\n",
    "header_large += ' spec_contrast_mean spec_contrast_std'\n",
    "header_large += ' spec_rolloff_mean spec_rolloff_std'\n",
    "header_large += ' spec_bandwidth_mean spec_bandwidth_std'\n",
    "header_large += ' spectral_flatness_mean spectral_flatness_std'\n",
    "    \n",
    "# Add tonnetz to header\n",
    "for i in range(1, 7):\n",
    "    header_large += f' tonnetz_mean_{i}'\n",
    "    header_large += f' tonnetz_std_{i}'\n",
    "    \n",
    "# Add tones to header\n",
    "pitches = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n",
    "for i in range(0, 12):\n",
    "    header_large += f' {pitches[i]}'\n",
    "\n",
    "# Add mfccs to header\n",
    "for i in range(1, 13):\n",
    "    header_large += f' mfcc_mean_{i+1}'\n",
    "    header_large += f' mfcc_std_{i+1}'\n",
    "    \n",
    "header_large = header_large.split()\n",
    "print(header_large)\n",
    "print(len(header_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466154a1-41a3-494f-ae04-bd4978aef2af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Structure analysis. Find segment of song to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e9196-d8c6-440f-b9dd-2850a98f735f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Music folder path\n",
    "rootfolder = \"track_library\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f479f-5faa-4b11-8766-e5a815cc43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a minumum segment duration\n",
    "extract_window_duration = 30\n",
    "min_duration = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7692e-4969-46e5-839a-bbc71be6b619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Path to directory with tracks\n",
    "lst = list(set(os.listdir(f'{rootfolder}')) - {'desktop.ini', '.ipynb_checkpoints'})\n",
    "\n",
    "# List of breakpoints\n",
    "startingpoints = []\n",
    "\n",
    "# Select a breakpoint for each track from where extraction should start\n",
    "for track in lst:\n",
    "    \n",
    "    # Print what track and load it\n",
    "    print(f'Analysing: {track}')  \n",
    "    trackpath = f'{rootfolder}/{track}'\n",
    "    y, sr = librosa.load(trackpath, mono=True, sr=None, duration=None)\n",
    "    \n",
    "    # Select a breakpoint\n",
    "    selected_breakpoint = extractStartingPoint(y=y, sr=sr, trackpath=trackpath)\n",
    "\n",
    "    # Print the selected breakpoint\n",
    "    #print(f'Selected breakpoint at: {selected_breakpoint:5.2f} seconds into the track')\n",
    "    startingpoints.append(selected_breakpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603250b-83b3-4f93-b4f4-9386f252ff1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70f985-45d8-4558-9995-3d3a55c30e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calcualte features and store in csv file\n",
    "file_namee = 'data_69.csv'\n",
    "file = open(file_namee, 'w', newline='')\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    #writer.writerow(header)\n",
    "    writer.writerow(header_large)\n",
    "\n",
    "# Generate per track\n",
    "#for track in lst:\n",
    "for i in range (0, len(lst)):\n",
    "    trackpath = f'{rootfolder}/{lst[i]}'\n",
    "    track = lst[i]\n",
    "    y, sr = librosa.load(trackpath, mono=True, sr=None, offset=startingpoints[i], duration=30)\n",
    "    \n",
    "    # Extract data\n",
    "    extractTrackDataAndWriteToCSV_2(y, sr, file_namee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09870945-2c24-4fe5-b469-ca8a7f1b120a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data from data.csv and display it\n",
    "data = pd.read_csv('data_69.csv') # data = pd.read_csv('data.csv')\n",
    "InitialValuesTable = copy.copy(data)\n",
    "data_track_names = data['track_name']\n",
    "data.drop(['track_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ea69a-7358-439a-b47f-bc1a0d52a6ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295fdb31-b26d-405a-8437-918e2225d304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardise the data \n",
    "data[data.columns] = scaler.fit_transform(data[data.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cde0ab-247c-4d43-aa0b-5475a814693a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaled Table | used as input for DR techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ddf9e-70f8-479f-b223-241c76569689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PRINT THE STANDARDISED DATA\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c3dd3-79b8-4f74-a457-aca90867b7a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inital values linked with track names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e55a7-1936-4af3-b76e-18feeeb210b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#InitialValuesTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451a6ec-39e4-48be-a370-33c7e915f250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_track_list = np.empty(shape=(0, 2))\n",
    "for i in range(0, data_track_names.shape[0]):\n",
    "    #print(f'{i}\\t{data_track_names[i]}')\n",
    "    id_track_list = np.append(id_track_list, np.array([[i, data_track_names[i]]]), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c8ecc-596d-4225-9a9a-ef072a9ec856",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DR - Explore extracted data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ead40-e1e8-4722-909c-9182643fce1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce632b-5774-459a-8774-68824072c7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TSNE\n",
    "X = data.copy()\n",
    "X_embedded = sklearn.manifold.TSNE(n_components=2, perplexity=5, learning_rate=200, init='pca').fit_transform(X)\n",
    "\n",
    "# Create empty array\n",
    "categories = np.zeros(X.shape[0])\n",
    "\n",
    "groups = np.array(['Outliers','Sub-cluster 1','Sub-cluster 2','Sub-cluster 3','Sub-cluster 4','Sub-cluster 5','Sub-cluster 6',\n",
    "                   'Sub-cluster 7','Sub-cluster 8','Sub-cluster 9','Sub-cluster 10','Sub-cluster 11'])\n",
    "colormap = np.array(['black','darkred','green','blue','yellow','cyan','purple','orangered','pink','lime','sienna','teal'])\n",
    "for id_track in id_track_list:\n",
    "    idx = int(id_track[0])\n",
    "    name = id_track[1]\n",
    "    if (str(name[:1]) == \"x\"):\n",
    "        categories[idx] = 0\n",
    "    elif (str(name[:3]) == \"c1_\"):\n",
    "        categories[idx] = 1\n",
    "    elif (str(name[:3]) == \"c2_\"):\n",
    "        categories[idx] = 2\n",
    "    elif (str(name[:3]) == \"c3_\"):\n",
    "        categories[idx] = 3\n",
    "    elif (str(name[:3]) == \"c4_\"):\n",
    "        categories[idx] = 4\n",
    "    elif (str(name[:3]) == \"c5_\"):\n",
    "        categories[idx] = 5\n",
    "    elif (str(name[:3]) == \"c6_\"):\n",
    "        categories[idx] = 6\n",
    "    elif (str(name[:3]) == \"c7_\"):\n",
    "        categories[idx] = 7\n",
    "    elif (str(name[:3]) == \"c8_\"):\n",
    "        categories[idx] = 8\n",
    "    elif (str(name[:3]) == \"c9_\"):\n",
    "        categories[idx] = 9\n",
    "    elif (str(name[:3]) == \"c10\"):\n",
    "        categories[idx] = 10\n",
    "    elif (str(name[:3]) == \"c11\"):\n",
    "        categories[idx] = 11\n",
    "    \n",
    "\n",
    "plt.scatter(X_embedded[:,0], X_embedded[:,1], s=150, c=colormap[categories.astype(int)])\n",
    "\n",
    "# Add Legend\n",
    "items = []\n",
    "for i in range(0,groups.size):\n",
    "    items.append(Line2D([0], [0], marker='o', color='whitesmoke', label=groups[i], markerfacecolor=colormap[i], markersize=10))\n",
    "    plt.legend(handles=items)\n",
    "    \n",
    "# Add annotations\n",
    "for i in range(0, data_track_names.shape[0]):\n",
    "    plt.text(X_embedded[i,0]+2.3, X_embedded[i,1]-1.5, (id_track_list[i][1])[:-4] , horizontalalignment='left', size=12, color='black', weight='semibold')\n",
    "\n",
    "plt.title('T-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2035425-dfcb-4c6a-b3fa-cc9193788deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.copy()\n",
    "\n",
    "# Perform DR down to 3D \n",
    "X_embedded = sklearn.manifold.TSNE(n_components=3, perplexity=5, learning_rate=10, init='pca').fit_transform(X)\n",
    "\n",
    "# creating a list of column names\n",
    "column_values = ['x', 'y', 'z']\n",
    "  \n",
    "# creating the dataframe\n",
    "df = pd.DataFrame(data = X_embedded, columns = column_values)\n",
    "\n",
    "trace = go.Scatter3d(x=df.x, y=df.y, z=df.z, mode='markers', marker=dict(size=10, color=colormap[categories.astype(int)], opacity=0.7))\n",
    "data_ = [trace]\n",
    "\n",
    "#trace_0 = go.Scatter3d(x=(df.x).iloc[0:11], y=(df.y).iloc[0:11], z=(df.z).iloc[0:11], mode='markers', marker=dict(size=10, color=colormap[0], opacity=0.75), name='Outliers')\n",
    "#trace_1 = go.Scatter3d(x=(df.x).iloc[11:16], y=(df.y).iloc[11:16], z=(df.z).iloc[11:16], mode='markers', marker=dict(size=10, color=colormap[1], opacity=0.75), name='Sub Cluster 1')\n",
    "#trace_2 = go.Scatter3d(x=(df.x).iloc[16:22], y=(df.y).iloc[16:22], z=(df.z).iloc[16:22], mode='markers', marker=dict(size=10, color=colormap[2], opacity=0.75), name='Sub Cluster 2')\n",
    "#trace_3 = go.Scatter3d(x=(df.x).iloc[22:29], y=(df.y).iloc[22:29], z=(df.z).iloc[22:29], mode='markers', marker=dict(size=10, color=colormap[3], opacity=0.75), name='Sub Cluster 3')\n",
    "#trace_4 = go.Scatter3d(x=(df.x).iloc[31:41], y=(df.y).iloc[31:41], z=(df.z).iloc[31:41], mode='markers', marker=dict(size=10, color=colormap[4], opacity=0.75), name='')\n",
    "#trace_5 = go.Scatter3d(x=(df.x).iloc[41:51], y=(df.y).iloc[41:51], z=(df.z).iloc[41:51], mode='markers', marker=dict(size=10, color=colormap[5], opacity=0.75), name='')\n",
    "#trace_6 = go.Scatter3d(x=(df.x).iloc[51:61], y=(df.y).iloc[51:61], z=(df.z).iloc[51:61], mode='markers', marker=dict(size=10, color=colormap[6], opacity=0.75), name='')\n",
    "#trace_7 = go.Scatter3d(x=(df.x).iloc[61:71], y=(df.y).iloc[61:71], z=(df.z).iloc[61:71], mode='markers', marker=dict(size=10, color=colormap[7], opacity=0.75), name='')\n",
    "#trace_8 = go.Scatter3d(x=(df.x).iloc[71:81], y=(df.y).iloc[71:81], z=(df.z).iloc[71:81], mode='markers', marker=dict(size=10, color=colormap[8], opacity=0.75), name='')\n",
    "#trace_9 = go.Scatter3d(x=(df.x).iloc[81:91], y=(df.y).iloc[81:91], z=(df.z).iloc[81:91], mode='markers', marker=dict(size=10, color=colormap[9], opacity=0.75), name='')\n",
    "#trace_10 = go.Scatter3d(x=(df.x).iloc[91:101], y=(df.y).iloc[91:101], z=(df.z).iloc[91:101], mode='markers', marker=dict(size=10, color=colormap[10], opacity=0.75), name='')\n",
    "#data_ = [trace_0, trace_1,  trace_2,  trace_3,]# trace_4, trace_5, trace_6, trace_7, trace_8, trace_9, trace_10]\n",
    "\n",
    "\n",
    "\n",
    "layout = go.Layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig = go.Figure(data=data_, layout=layout)\n",
    "fig.update_layout(autosize=False, width=800, height=800,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca41f0a-4cec-43d5-ae55-498edd965db5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PaCMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691273fe-c2de-4e22-9d82-0499f4330c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pacmap\n",
    "import pacmap\n",
    "\n",
    "X2 = data.copy()\n",
    "\n",
    "X2_embedded = pacmap.PaCMAP(n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0)\n",
    "X2_transformed = X2_embedded.fit_transform(X2.values, init='pca')\n",
    "\n",
    "# Create empty array\n",
    "categories = np.zeros(X.shape[0])\n",
    "\n",
    "groups = np.array(['Outliers','Sub-cluster 1','Sub-cluster 2','Sub-cluster 3','Sub-cluster 4','Sub-cluster 5','Sub-cluster 6',\n",
    "                   'Sub-cluster 7','Sub-cluster 8','Sub-cluster 9','Sub-cluster 10','Sub-cluster 11'])\n",
    "colormap = np.array(['black','darkred','green','blue','yellow','cyan','purple','orangered','pink','lime','sienna','teal'])\n",
    "for id_track in id_track_list:\n",
    "    idx = int(id_track[0])\n",
    "    name = id_track[1]\n",
    "    if (str(name[:1]) == \"x\"):\n",
    "        categories[idx] = 0\n",
    "    elif (str(name[:3]) == \"c1_\"):\n",
    "        categories[idx] = 1\n",
    "    elif (str(name[:3]) == \"c2_\"):\n",
    "        categories[idx] = 2\n",
    "    elif (str(name[:3]) == \"c3_\"):\n",
    "        categories[idx] = 3\n",
    "    elif (str(name[:3]) == \"c4_\"):\n",
    "        categories[idx] = 4\n",
    "    elif (str(name[:3]) == \"c5_\"):\n",
    "        categories[idx] = 5\n",
    "    elif (str(name[:3]) == \"c6_\"):\n",
    "        categories[idx] = 6\n",
    "    elif (str(name[:3]) == \"c7_\"):\n",
    "        categories[idx] = 7\n",
    "    elif (str(name[:3]) == \"c8_\"):\n",
    "        categories[idx] = 8\n",
    "    elif (str(name[:3]) == \"c9_\"):\n",
    "        categories[idx] = 9\n",
    "    elif (str(name[:3]) == \"c10\"):\n",
    "        categories[idx] = 10\n",
    "    elif (str(name[:3]) == \"c11\"):\n",
    "        categories[idx] = 11\n",
    "\n",
    "# Scatter\n",
    "plt.scatter(X2_transformed[:,0], X2_transformed[:,1], s=150, c=colormap[categories.astype(int)])\n",
    "\n",
    "# Add Legend\n",
    "items = []\n",
    "for i in range(0, groups.size):\n",
    "    items.append(Line2D([0], [0], marker='o', color='whitesmoke', label=groups[i], markerfacecolor=colormap[i], markersize=10))\n",
    "    plt.legend(handles=items)\n",
    "    \n",
    "# Add annotations\n",
    "for i in range(0, data_track_names.shape[0]):\n",
    "    plt.text(X2_transformed[i,0]+0.05, X2_transformed[i,1]-0.04, (id_track_list[i][1])[:-4], horizontalalignment='left', size=12, color='black', weight='semibold')\n",
    "    \n",
    "plt.title('PaCMAP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf072a-eeca-4179-bcce-a7a0178cd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = data.copy()\n",
    "\n",
    "# Perform DR down to 3D\n",
    "X2_embedded = pacmap.PaCMAP(n_components=3, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0)\n",
    "X2_transformed = X2_embedded.fit_transform(X2.values, init='pca')\n",
    "\n",
    "# creating a list of column names\n",
    "column_values = ['x', 'y', 'z']\n",
    "  \n",
    "# creating the dataframe\n",
    "df = pd.DataFrame(data = X2_transformed, columns = column_values)\n",
    "\n",
    "trace = go.Scatter3d(x=df.x, y=df.y, z=df.z, mode='markers', marker=dict(size=10, color=colormap[categories.astype(int)], opacity=0.8))\n",
    "data_ = [trace]\n",
    "layout = go.Layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig = go.Figure(data=data_, layout=layout)\n",
    "fig.update_layout(autosize=False, width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0651c4a-e605-4e41-90f7-4f6aa3d0cb2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Similiraty Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2f5e6-c4c3-43e2-aa98-7572d6facf79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# = X_embedded.copy()\n",
    "X = X2_transformed.copy()\n",
    "X = \n",
    "\n",
    "# Current song index\n",
    "current_song_idx = 0\n",
    "\n",
    "similarity_metrics = []\n",
    "# Compute distances from selected(\"playing\") track to the others\n",
    "for i in range(0, X.shape[0]):\n",
    "    dist = np.linalg.norm(X[current_song_idx] - X[i])\n",
    "    similarity_metrics.append([id_track_list[i][1], dist])\n",
    "    \n",
    "similarity_metrics.sort(key=lambda x: x[1])\n",
    "\n",
    "for item in similarity_metrics:\n",
    "    print (\"Track: \",item[0], \"\\tDistance: \", item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1160f4-4272-4198-ba2a-5f08acdb50b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of tracks / size of embedded array\n",
    "X_size = X.shape[0]\n",
    "\n",
    "sim_matrix = np.zeros(shape=(0, X_size))\n",
    "\n",
    "# Comute 2d array of distances\n",
    "for i in range(0, X.shape[0]):\n",
    "    similarity_metrics = []\n",
    "    for j in range(0, X.shape[0]):\n",
    "        dist = np.linalg.norm(X[i] - X[j])\n",
    "        similarity_metrics.append(dist)\n",
    "    sim_matrix = np.append(sim_matrix, np.array([similarity_metrics]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4f7f1-6318-4aa0-a01c-41e74d2f987f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.matshow(sim_matrix)\n",
    "\n",
    "for (i, j), z in np.ndenumerate(sim_matrix):\n",
    "    ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center', bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n",
    "    \n",
    "ax.set_ylabel('tracks')\n",
    "ax.set_xlabel('tracks')\n",
    "ax.set_title('Distances between tracks')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81747f7-4221-432a-a323-6617c861a556",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421ff9c-ded5-43d1-a594-ac04ff63560d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DEAM Dataset on AROUSAL and VALENCE with my extracted features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e71ab-3ad4-4f4c-8181-441887ed1d0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract annotations and what quarter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d9f35-086b-475c-a07c-7d211dedf6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deam anotaions\n",
    "file_1 = r\"deam\\static_annotations_averaged_songs_1_2000.csv\"\n",
    "file_2 = r\"deam\\static_annotations_averaged_songs_2000_2058.csv\"\n",
    "annotations = pd.read_csv(file_1)\n",
    "    \n",
    "# Remove unneccesay columns\n",
    "col_info = list(enumerate(annotations.columns))\n",
    "annotations.drop(columns=[col_info[2][1]], inplace=True)  # Remove valance_std\n",
    "\n",
    "col_info = list(enumerate(annotations.columns))\n",
    "annotations.drop(columns=[col_info[3][1]], inplace=True)  # Remove arousal_std\n",
    "\n",
    "# Rename columns\n",
    "annotations.rename(columns = {' valence_mean':'valence'}, inplace = True)\n",
    "annotations.rename(columns = {' arousal_mean':'arousal'}, inplace = True)\n",
    "\n",
    "# Mean Normalize values (center around 0)\n",
    "annotations['valence'] -= 5\n",
    "annotations['arousal'] -= 5\n",
    "\n",
    "def feeling_map(t):\n",
    "    if t['valence'] >= 0 and t['arousal'] >= 0:\n",
    "        return 0 \n",
    "    if t['valence'] >= 0 and t['arousal'] < 0:\n",
    "        return 1\n",
    "    if t['valence'] < 0 and t['arousal'] >= 0:\n",
    "        return 2 \n",
    "    if t['valence'] < 0 and t['arousal'] < 0:\n",
    "        return 3\n",
    "\n",
    "# Add a new column\n",
    "annotations[\"Feeling\"] = annotations.apply(feeling_map, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a9c79-176a-47a2-a07b-e326fae89df1",
   "metadata": {},
   "source": [
    "High-Valance-High-Arousal (0, \"Happy\")  \n",
    "High-Valance-Low-Arousal (1, \"Calm\")  \n",
    "Low-Valance-High-Arousal (2, \"Angry\")  \n",
    "Low-Valance-Low-Arousal (3, \"Sad\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ebdf1-7d11-40d0-9d79-88307076a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63b9cd-714d-4bdd-9830-89309297826e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract features from the deam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31348d9-9d8f-4aad-beaa-6a4ed6b20c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Path to directory with tracks\n",
    "excerpts = list(set(os.listdir(\"deam/MEMD_audio/\")) - {'desktop.ini', '.ipynb_checkpoints'})\n",
    "\n",
    "# Calcualte features and store in csv file\n",
    "file = open('my_deam_features_x.csv', 'w', newline='')\n",
    "with file:\n",
    "    writer = csv.writer(file) \n",
    "    writer.writerow(header_large)\n",
    "\n",
    "number_ = 0\n",
    "# Loop through the deam database\n",
    "for excerpt in excerpts:\n",
    "    songpath = f'deam/MEMD_audio/{excerpt}'\n",
    "    track = excerpt\n",
    "    y, sr = librosa.load(songpath, mono=True, sr=None)\n",
    "    # Extract data \n",
    "    extractTrackDataAndWriteToCSV_2(y, sr, 'my_deam_features_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e02954-4cba-4836-97f5-32c05ef4f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from data.csv and display it\n",
    "deam_data = pd.read_csv('my_deam_features_69.csv')\n",
    "InitialValuesTable = copy.copy(deam_data)\n",
    "trackNames = deam_data['track_name']\n",
    "\n",
    "# Preprocess data\n",
    "# Reformat from .mp3(string) to int\n",
    "for i in range (0, deam_data.shape[0]):            \n",
    "    track_id = int(deam_data.at[i, 'track_name'].replace('.mp3',''))\n",
    "    deam_data.at[i, 'track_name'] = track_id\n",
    "\n",
    "# Cast column to int in order to sort it\n",
    "deam_data = deam_data.astype({'track_name': 'int32'})\n",
    "\n",
    "# Sort in order to align excerpts with the annotated data\n",
    "deam_data.sort_values(by=['track_name'], inplace=True)\n",
    "\n",
    "# Drop any tracks with id > 2000\n",
    "deam_data = deam_data[deam_data['track_name'] <= 2000]\n",
    "\n",
    "# Drop the 'track_name' column\n",
    "deam_data.drop(['track_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b63434-302c-424b-972d-8b0b52ffc2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "deam_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b568e20-76fe-4b1d-b12d-0516b3bc0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "deam_data_valence = annotations['valence']\n",
    "deam_data_arousal = annotations['arousal']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae6d8f-2fdf-4d9a-a2bb-16b2ab94195b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regression - Valence and Arousal - Seperate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeff9f2-06a9-44da-b7fd-05ad76246613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = deam_data.copy()\n",
    "Y_val = deam_data_valence\n",
    "Y_aro = deam_data_arousal\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets with valence as target\n",
    "X_train_valence, X_val_and_test_valence, Y_train_valence, Y_val_and_test_valence = train_test_split(X_scale, Y_val, test_size=0.3)\n",
    "X_val_valence, X_test_valence, Y_val_valence, Y_test_valence =  train_test_split(X_val_and_test_valence, Y_val_and_test_valence, test_size=0.5)\n",
    "\n",
    "# Split into train and test sets with arousal as target\n",
    "X_train_arousal, X_val_and_test_arousal, Y_train_arousal, Y_val_and_test_arousal = train_test_split(X_scale, Y_aro, test_size=0.3)\n",
    "X_val_arousal, X_test_arousal, Y_val_arousal, Y_test_arousal =  train_test_split(X_val_and_test_arousal, Y_val_and_test_arousal, test_size=0.5)\n",
    "\n",
    "# X_train contains 70% of total dataset\n",
    "print(X_train_valence.shape)\n",
    "# X_test contains 30% of total dataset\n",
    "print(X_test_valence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709d48a-5e5f-4df0-8aed-4c31b5f0ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on valence\n",
    "lr_valence = LinearRegression()\n",
    "lr_valence.fit(X_train_valence, Y_train_valence)\n",
    "\n",
    "# Train on arousal\n",
    "lr_arousal = LinearRegression()\n",
    "lr_arousal.fit(X_train_arousal, Y_train_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8c823-0dfe-4ef8-9c2c-77f67ad021a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the intercept\n",
    "print(\"Valence intercept: \", lr_valence.intercept_)\n",
    "print(\"Arousal intercept: \", lr_arousal.intercept_)\n",
    "\n",
    "#Coefficients\n",
    "coeff_val_df = pd.DataFrame(lr_valence.coef_, X.columns, columns=['Coefficient'])\n",
    "#coeff_val_df\n",
    "coeff_aro_df = pd.DataFrame(lr_arousal.coef_, X.columns, columns=['Coefficient'])\n",
    "#coeff_aro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa8ad4-d629-40da-b34c-19886a2f2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = lr_valence.predict(X_test_valence)\n",
    "pred_aro = lr_arousal.predict(X_test_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b2893-91d1-4110-8957-fc5ad4af113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test_valence, pred_val)\n",
    "plt.title(\"Predicting valence scores with linear regression model\")\n",
    "plt.xlabel(\"valence\")\n",
    "plt.ylabel(\"Predicted Valence\")\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028fcd82-98b9-4d86-b5f6-6d5f5d22e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Valence RMSE:', np.sqrt(metrics.mean_squared_error(Y_test_valence, pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9435f8e-a53c-4091-87f9-82a4cba785c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test_arousal, pred_aro)\n",
    "plt.title(\"Predicting arousal scores with linear regression model\")\n",
    "plt.xlabel(\"Arousal\")\n",
    "plt.ylabel(\"Predicted Arousal\")\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ead942-96ef-4271-a4fd-7118853a6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Arousal RMSE:', np.sqrt(metrics.mean_squared_error(Y_test_arousal, pred_aro)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec41608-126d-4463-b828-b4cdf498ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valence_reg = lr_valence.predict(data)\n",
    "pred_arousal_reg = lr_arousal.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a78e0-025e-46c7-8711-01a518f015bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neural network - regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5e061-f81d-4686-9a01-6d60625d272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries of tensorflow to build neurral networks.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b91b00-d4c6-4349-bb94-c04aa837121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = deam_data.copy()\n",
    "Y_valence = deam_data_valence\n",
    "Y_arousal = deam_data_arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69311dac-ed9b-4f7c-b464-4f77d00cbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc10d8-b747-4bf6-9a49-518c3601ebf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neural network for predicting valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a104f-a834-4719-8310-566d701aaa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test =  train_test_split(X_scale, Y_valence, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test =  train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935048c6-536b-4d5d-b911-fde56788fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model_1 = Sequential([\n",
    "    Dense(32, input_shape=(X.shape[1],), activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='linear'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cfa9c-f230-4c0f-8d17-b636439f7e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "learning_rate = 0.01\n",
    "model_1.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285967de-7571-4ba4-afad-ddc31b8fbede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weigths of the model\n",
    "model_1.save_weights('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e3767-cc5c-4295-93be-cd254a234cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1.load_weights('model_1.h5') # reset\n",
    "hist = model_1.fit(X_train, Y_train, validation_data=(X_val,Y_val), verbose=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02a4ab-8db1-4ecc-b8f1-ead49ec31dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-4: Evalution\n",
    "model_1.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb2673-139b-4687-908e-3d2f70d11874",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_1.predict(X_test)\n",
    "plt.scatter(Y_test, pred)\n",
    "plt.title(\"Predicting valence scores with neural network model\")\n",
    "plt.xlabel(\"Valence\")\n",
    "plt.ylabel(\"Predicted Valence\")\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee76d0aa-9a95-4b4d-b8e5-c0e99dc5ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valence = model_1.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff600e-4859-4de2-8dd0-9109a99b91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['root_mean_squared_error'])\n",
    "plt.plot(hist.history['val_root_mean_squared_error'])\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be39d7-d738-4c97-92bd-8d34ef7b442b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neural network for predecting arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b6154a-3e62-4c71-a543-895c2a97f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test =  train_test_split(X_scale, Y_arousal, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test =  train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d4a71-c350-4f31-99da-93cbd2a86aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model_2 = Sequential([\n",
    "    Dense(32, input_shape=(X.shape[1],), activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='linear'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9b1bff-774e-492d-835e-8b78c2c17448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "learning_rate = 0.01\n",
    "model_2.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec7487-40b2-4dc3-9932-b72887f5c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weigths of the model\n",
    "model_2.save_weights('model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d587fe4-5397-4d98-a934-aa70689d55c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_2.load_weights('model_2.h5')\n",
    "hist_2 = model_2.fit(X_train, Y_train, validation_data=(X_val,Y_val), verbose=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3733e-564f-4e08-9ca2-d14ae20400cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-4: Evalution\n",
    "model_2.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a712b-ea27-4ced-802e-6370e33afb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_2.predict(X_test)\n",
    "plt.scatter(Y_test, pred)\n",
    "plt.title(\"Predicting arousal scores with neural network model\")\n",
    "plt.xlabel(\"Arousal\")\n",
    "plt.ylabel(\"Predicted Arousal\")\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8e82f-629e-4ab5-a329-4aa7235d05bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arousal = model_2.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ddec3-8318-43f8-902c-86257e580a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_2.history['root_mean_squared_error'])\n",
    "plt.plot(hist_2.history['val_root_mean_squared_error'])\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17efea-3ec7-4e9d-9840-c1acecd525df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read track names from library to estimate valance and arousal\n",
    "data2 = pd.read_csv('data_69.csv')\n",
    "data_track_names = data2['track_name']\n",
    "\n",
    "# Turn into dataframe and add column name\n",
    "data_track_names = data_track_names.to_frame()\n",
    "data_track_names.columns = ['Track']\n",
    "\n",
    "# Flatten the list of predicted valence and arousal\n",
    "flat_pred_valence = [item for sublist in pred_valence for item in sublist]\n",
    "flat_pred_arousal = [item for sublist in pred_arousal for item in sublist]\n",
    "\n",
    "\n",
    "def feeling_map_1(t):\n",
    "    if t['Valence-nn'] >= 0 and t['Arousal-nn'] >= 0:\n",
    "        return 0 \n",
    "    if t['Valence-nn'] >= 0 and t['Arousal-nn'] < 0:\n",
    "        return 1\n",
    "    if t['Valence-nn'] < 0 and t['Arousal-nn'] >= 0:\n",
    "        return 2 \n",
    "    if t['Valence-nn'] < 0 and t['Arousal-nn'] < 0:\n",
    "        return 3\n",
    "    \n",
    "def feeling_map_2(t):\n",
    "    if t['Valence-reg'] >= 0 and t['Arousal-reg'] >= 0:\n",
    "        return 0 \n",
    "    if t['Valence-reg'] >= 0 and t['Arousal-reg'] < 0:\n",
    "        return 1\n",
    "    if t['Valence-reg'] < 0 and t['Arousal-reg'] >= 0:\n",
    "        return 2 \n",
    "    if t['Valence-reg'] < 0 and t['Arousal-reg'] < 0:\n",
    "        return 3\n",
    "\n",
    "# Append regression and nerual netork results to dataframe\n",
    "data_track_names['Valence-nn'] = flat_pred_valence\n",
    "data_track_names['Arousal-nn'] = flat_pred_arousal\n",
    "data_track_names[\"Feeling-nn\"] = data_track_names.apply(feeling_map_1, axis=1)\n",
    "data_track_names['Valence-reg'] = pred_valence_reg.tolist()\n",
    "data_track_names['Arousal-reg'] = pred_arousal_reg.tolist()\n",
    "data_track_names[\"Feeling-reg\"] = data_track_names.apply(feeling_map_2, axis=1)\n",
    "    \n",
    "# Append values to original data as well\n",
    "data2['valence-nn'] = flat_pred_valence\n",
    "data2['arousal-nn'] = data_track_names['Arousal-nn']\n",
    "data2['valence-reg'] = data_track_names['Valence-reg']\n",
    "data2['arousal-reg'] = data_track_names['Arousal-reg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7ecc0-1fe6-47b2-956b-f5eec0293ccf",
   "metadata": {},
   "source": [
    "High-Valance-High-Arousal (0, \"Happy\") - High-Valance-Low-Arousal (1, \"Calm\") - Low-Valance-High-Arousal (2, \"Angry\") - Low-Valance-Low-Arousal (3, \"Sad\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7b881-710f-4089-adbf-75fc72a4a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by various values\n",
    "#data_track_names.sort_values(by=['Valence-nn'])\n",
    "data_track_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c557f83-4c6a-44a5-96c1-13615b688cc0",
   "metadata": {},
   "source": [
    "### Colorcode according to feelings and do DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509ced2-4573-493f-9221-46b56c80c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE\n",
    "X = data.copy()\n",
    "X_embedded = sklearn.manifold.TSNE(n_components=2, perplexity=8, learning_rate=200, init='pca').fit_transform(X)\n",
    "\n",
    "# Create empty array\n",
    "categories = np.zeros(X.shape[0])\n",
    "\n",
    "groups = np.array(['0: Happy', '1: Calm', '2: Angry', '3: Sad'])\n",
    "colormap = np.array(['forestgreen', 'lightblue', 'red', 'blue'])\n",
    "\n",
    "feelings = data_track_names['Feeling-nn']\n",
    "for i in range(0, X.shape[0]):\n",
    "    feel = feelings.iloc[i]\n",
    "    if (feel == 0):\n",
    "        categories[i] = 0\n",
    "    elif (feel == 1):\n",
    "        categories[i] = 1\n",
    "    elif (feel == 2):\n",
    "        categories[i] = 2\n",
    "    elif (feel == 3):\n",
    "        categories[i] = 3\n",
    "\n",
    "plt.scatter(X_embedded[:,0], X_embedded[:,1], s=150, c=colormap[categories.astype(int)])\n",
    "\n",
    "# Add Legend\n",
    "items = []\n",
    "for i in range(0,groups.size):\n",
    "    items.append(Line2D([0], [0], marker='o', color='whitesmoke', label=groups[i], markerfacecolor=colormap[i], markersize=10))\n",
    "    plt.legend(handles=items)\n",
    "    \n",
    "# Add annotations\n",
    "for i in range(0, data_track_names.shape[0]):\n",
    "    plt.text(X_embedded[i,0]+2.3, X_embedded[i,1]-1.5, (id_track_list[i][1])[:-4] , horizontalalignment='left', size=12, color='black', weight='semibold')\n",
    "\n",
    "plt.title('T-SNE - Neural Network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae58c7-c16c-4270-9f50-2d435cb45adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE\n",
    "X = data.copy()\n",
    "X_embedded = sklearn.manifold.TSNE(n_components=2, perplexity=8, learning_rate=200, init='pca').fit_transform(X)\n",
    "\n",
    "# Create empty array\n",
    "categories = np.zeros(X.shape[0])\n",
    "\n",
    "groups = np.array(['0: Happy', '1: Calm', '2: Angry', '3: Sad'])\n",
    "colormap = np.array(['forestgreen', 'lightblue', 'red', 'blue'])\n",
    "\n",
    "feelings = data_track_names['Feeling-reg']\n",
    "for i in range(0, X.shape[0]):\n",
    "    feel = feelings.iloc[i]\n",
    "    if (feel == 0):\n",
    "        categories[i] = 0\n",
    "    elif (feel == 1):\n",
    "        categories[i] = 1\n",
    "    elif (feel == 2):\n",
    "        categories[i] = 2\n",
    "    elif (feel == 3):\n",
    "        categories[i] = 3\n",
    "\n",
    "plt.scatter(X_embedded[:,0], X_embedded[:,1], s=150, c=colormap[categories.astype(int)])\n",
    "\n",
    "# Add Legend\n",
    "items = []\n",
    "for i in range(0,groups.size):\n",
    "    items.append(Line2D([0], [0], marker='o', color='whitesmoke', label=groups[i], markerfacecolor=colormap[i], markersize=10))\n",
    "    plt.legend(handles=items)\n",
    "    \n",
    "# Add annotations\n",
    "for i in range(0, data_track_names.shape[0]):\n",
    "    plt.text(X_embedded[i,0]+2.3, X_embedded[i,1]-1.5, (id_track_list[i][1])[:-4] , horizontalalignment='left', size=12, color='black', weight='semibold')\n",
    "\n",
    "plt.title('T-SNE - Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0f315-5114-45b3-894b-add52d6101f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PaCMAP\n",
    "X2 = data.copy()\n",
    "X2_embedded = pacmap.PaCMAP(n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0)\n",
    "X2_transformed = X2_embedded.fit_transform(X2.values, init='pca')\n",
    "\n",
    "# Create empty array\n",
    "categories = np.zeros(X2.shape[0])\n",
    "\n",
    "groups = np.array(['0: Happy', '1: Calm', '2: Angry', '3: Sad'])\n",
    "colormap = np.array(['forestgreen', 'lightblue', 'red', 'blue'])\n",
    "\n",
    "feelings = data_track_names['Feeling-nn']\n",
    "for i in range(0, X.shape[0]):\n",
    "    feel = feelings.iloc[i]\n",
    "    if (feel == 0):\n",
    "        categories[i] = 0\n",
    "    elif (feel == 1):\n",
    "        categories[i] = 1\n",
    "    elif (feel == 2):\n",
    "        categories[i] = 2\n",
    "    elif (feel == 3):\n",
    "        categories[i] = 3\n",
    "\n",
    "plt.scatter(X2_transformed[:,0], X2_transformed[:,1], s=150, c=colormap[categories.astype(int)])\n",
    "\n",
    "# Add Legend\n",
    "items = []\n",
    "for i in range(0,groups.size):\n",
    "    items.append(Line2D([0], [0], marker='o', color='whitesmoke', label=groups[i], markerfacecolor=colormap[i], markersize=10))\n",
    "    plt.legend(handles=items)\n",
    "    \n",
    "# Add annotations\n",
    "for i in range(0, data_track_names.shape[0]):\n",
    "    plt.text(X2_transformed[i,0]+0.05, X2_transformed[i,1]-0.04, (id_track_list[i][1])[:-4], horizontalalignment='left', size=12, color='black', weight='semibold')\n",
    "\n",
    "plt.title('PaCMAP - Neural Network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b4309-a26e-4898-afc2-6076a3c831f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PaCMAP\n",
    "X2 = data.copy()\n",
    "X2_embedded = pacmap.PaCMAP(n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0)\n",
    "X2_transformed = X2_embedded.fit_transform(X2.values, init='pca')\n",
    "\n",
    "# Create empty array\n",
    "categories = np.zeros(X2.shape[0])\n",
    "\n",
    "groups = np.array(['0: Happy', '1: Calm', '2: Angry', '3: Sad'])\n",
    "colormap = np.array(['forestgreen', 'lightblue', 'red', 'blue'])\n",
    "\n",
    "feelings = data_track_names['Feeling-reg']\n",
    "for i in range(0, X.shape[0]):\n",
    "    feel = feelings.iloc[i]\n",
    "    if (feel == 0):\n",
    "        categories[i] = 0\n",
    "    elif (feel == 1):\n",
    "        categories[i] = 1\n",
    "    elif (feel == 2):\n",
    "        categories[i] = 2\n",
    "    elif (feel == 3):\n",
    "        categories[i] = 3\n",
    "\n",
    "plt.scatter(X2_transformed[:,0], X2_transformed[:,1], s=150, c=colormap[categories.astype(int)])\n",
    "\n",
    "# Add Legend\n",
    "items = []\n",
    "for i in range(0,groups.size):\n",
    "    items.append(Line2D([0], [0], marker='o', color='whitesmoke', label=groups[i], markerfacecolor=colormap[i], markersize=10))\n",
    "    plt.legend(handles=items)\n",
    "    \n",
    "# Add annotations\n",
    "for i in range(0, data_track_names.shape[0]):\n",
    "    plt.text(X2_transformed[i,0]+0.05, X2_transformed[i,1]-0.04, (id_track_list[i][1])[:-4], horizontalalignment='left', size=12, color='black', weight='semibold')\n",
    "\n",
    "plt.title('PaCMAP - Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c5e4c-cc65-444d-b1e5-0664b95c1e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6726ae0-2f4a-4cc9-89db-87c889d81c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25b5162d-859b-48df-8ca9-86a9baf0efda",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classification with DEAM data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb452bd5-1b79-43f9-91f8-2dbe9d4e7d85",
   "metadata": {},
   "source": [
    "Using my extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81d510-f086-4bc2-9632-5afd450fe3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "my_features = deam_data.copy()\n",
    "feeling_label = annotations['Feeling']\n",
    "print(my_features.shape)\n",
    "print(feeling_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbb36a-5974-414f-8adb-d848e0d84c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the data \n",
    "my_features[my_features.columns] = scaler.fit_transform(my_features[my_features.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9fbd1-3afa-42be-b81c-ea25b312ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420eba2-f61e-4e58-9ca6-431cb9adffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(my_features, feeling_label, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5a2c1-b562-4eae-a101-e7aefd12f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3f9e5-6338-494b-8793-b76be464fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = knn.score(X_train, y_train)\n",
    "test_score = knn.score(X_test, y_test)\n",
    "\n",
    "# Round dwon the floats to two decimals\n",
    "train_score_rounded = round(train_score, 2)\n",
    "test_score_rounded = round(test_score, 2)\n",
    "\n",
    "print(\"training_score: \",train_score_rounded,\" test_score: \", test_score_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c26fb0-8df0-47fe-958c-c94bef53fec1",
   "metadata": {},
   "source": [
    "Using DEAM's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e17c60-e313-46dd-bf3d-1d6ed56da0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfObj = pd.DataFrame(columns=['User_ID', 'UserName', 'Action'])\n",
    "dfObj = pd.DataFrame()\n",
    "\n",
    "for ids in annotations['song_id']:\n",
    "    file = f'deam\\\\features\\{ids}.csv'\n",
    "    deam_features = pd.read_csv(file, sep = ';')\n",
    "    deam_features.drop('frameTime', inplace=True,axis=1)\n",
    "    deam_features = deam_features.mean(axis=0)\n",
    "    dfObj = dfObj.append(deam_features, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f4adb-24f1-4ed1-805d-1229786a7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the data \n",
    "dfObj[dfObj.columns] = scaler.fit_transform(dfObj[dfObj.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c2a44-deda-4b4c-9891-448e837343e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e278f-1bfd-4fb8-9e1d-5b42d10da2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deam_features = dfObj.copy()\n",
    "feeling_label = annotations['Feeling']\n",
    "print(deam_features.shape)\n",
    "print(feeling_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905a389-2d1c-4204-a753-cd6bb1654434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(deam_features, feeling_label, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38793d9b-a968-4577-9fe5-891030da64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944406d-8b61-4076-ad46-1fe92c6897a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = knn.score(X_train, y_train)\n",
    "test_score = knn.score(X_test, y_test)\n",
    "# Round dwon the floats to two decimals\n",
    "train_score_rounded = round(train_score, 2)\n",
    "test_score_rounded = round(test_score, 2)\n",
    "\n",
    "print(\"training_score: \",train_score_rounded,\" test_score: \", test_score_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01364870-3948-4dbb-a68b-c5d7aca57511",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 100 copies of a song with 10 incremental changes in 10 varying dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9fcc0-be48-4bb5-8288-ecf777ccd5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "filename = 'eval_69.csv'\n",
    "track = 'c1_1_ame.wav' # Full Intention - America (I Love America) (Full Length Vocal Mix)\n",
    "track_path = 'track_library/c1_1_ame.wav'\n",
    "\n",
    "# Load the track\n",
    "y_eval, sr_eval = librosa.load(track_path, mono=True, sr=None, duration=None)\n",
    "\n",
    "# select starting point for feature selection\n",
    "startpoint = extractStartingPoint(y_eval, sr_eval, track_path)\n",
    "\n",
    "# Reload track but only the selected segment\n",
    "y_eval, sr_eval = librosa.load(track_path, mono=True, sr=None, offset=startpoint, duration=30)\n",
    "\n",
    "print(f'Starting point selected at {startpoint} seconds into the track')\n",
    "\n",
    "from audiomentations import (\n",
    "    Compose,\n",
    "    LowPassFilter,\n",
    "    HighPassFilter,\n",
    ")\n",
    "\n",
    "# High pass filter\n",
    "hipass = 20\n",
    "augment = Compose([HighPassFilter(min_cutoff_freq=hipass, max_cutoff_freq=hipass, p = 1)])\n",
    "y_eval = augment(samples=y_eval, sample_rate=sr_eval)\n",
    "\n",
    "# low pass filter\n",
    "lopass = 20000\n",
    "augment = Compose([LowPassFilter(min_cutoff_freq=lopass, max_cutoff_freq=lopass, p = 1)])\n",
    "y_eval = augment(samples=y_eval, sample_rate=sr_eval)\n",
    "\n",
    "print(f'removed frequencies outside [20, 20000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad25ed0-e7eb-48ea-8a17-eef04def7928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from audiomentations import (\n",
    "    Compose,\n",
    "    LowPassFilter,\n",
    "    HighPassFilter,\n",
    "    AddGaussianNoise,\n",
    "    TanhDistortion,\n",
    ")\n",
    "\n",
    "from pedalboard import (\n",
    "    Pedalboard,\n",
    "    Reverb,\n",
    "    Phaser,\n",
    "    Chorus,\n",
    "    Bitcrush,\n",
    "    Gain,\n",
    ")\n",
    "\n",
    "#Select deature set: 0 = 39 feature data, 1 = 69 feature data\n",
    "feature_set = 1\n",
    "\n",
    "# Calcualte features and store in csv file\n",
    "file = open(filename, 'w', newline='')\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    if (feature_set == 0):\n",
    "        writer.writerow(header)\n",
    "    else:\n",
    "        writer.writerow(header_large)\n",
    "    \n",
    "track = f'Original'\n",
    "if (feature_set == 0):\n",
    "    extractTrackDataAndWriteToCSV(y_eval, sr_eval, filename)\n",
    "else:\n",
    "    extractTrackDataAndWriteToCSV_2(y_eval, sr_eval, filename)\n",
    "    \n",
    "# Dimension 1.1 - bpm up\n",
    "steps = np.arange(1.02, 1.10, 0.02)\n",
    "for i in range(0, 5):\n",
    "    y_dim_1 = pyrb.time_stretch(y_eval, sr_eval, steps[i])\n",
    "    track = f'bpm_up_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_1, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_1, sr_eval, filename)\n",
    "    \n",
    "# Dimension 1.2 - bpm down\n",
    "steps = np.arange(0.98, 0.88, -0.02)\n",
    "for i in range(0, 5):\n",
    "    y_dim_1 = pyrb.time_stretch(y_eval, sr_eval, steps[i])\n",
    "    track = f'bpm_down_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_1, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_1, sr_eval, filename)\n",
    "    \n",
    "print(\"Extracted dim 1/10\")\n",
    "    \n",
    "# Dimension 2.1 - Pitch up\n",
    "steps = np.arange(0.5, 3, 0.5)\n",
    "for i in range(0, 5):\n",
    "    y_dim_2 = pyrb.pitch_shift(y_eval, sr_eval, steps[i]) \n",
    "    track = f'pitch_up_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_2, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_2, sr_eval, filename)\n",
    "    \n",
    "# Dimension 2.2 - Pitch down\n",
    "steps = np.arange(-0.5, -3, -0.5)\n",
    "for i in range(0, 5):\n",
    "    y_dim_2 = pyrb.pitch_shift(y_eval, sr_eval, steps[i]) \n",
    "    track = f'pitch_down_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_2, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_2, sr_eval, filename)\n",
    "        \n",
    "print(\"Extracted dim 2/10\")\n",
    "    \n",
    "# Dimension 3 - High pass filter\n",
    "steps = np.arange(30, 330, 30)\n",
    "for i in range(0, 10):\n",
    "    augment = Compose([HighPassFilter(min_cutoff_freq=steps[i], max_cutoff_freq=steps[i], p = 1)])\n",
    "    y_dim_3 = augment(samples=y_eval, sample_rate=sr_eval)\n",
    "    track = f'hpfilter_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_3, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_3, sr_eval, filename)\n",
    "        \n",
    "print(\"Extracted dim 3/10\")\n",
    "\n",
    "# Dimension 4 - low pass filter\n",
    "steps = np.arange(19000, 14000, -500)\n",
    "for i in range(0, 10):\n",
    "    augment = Compose([LowPassFilter(min_cutoff_freq=steps[i], max_cutoff_freq=steps[i], p = 1)])\n",
    "    y_dim_4 = augment(samples=y_eval, sample_rate=sr_eval)\n",
    "    track = f'lpfilter_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_4, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_4, sr_eval, filename)\n",
    "    \n",
    "print(\"Extracted dim 4/10\")\n",
    "\n",
    "# Dimension 5 - Gaussion noise\n",
    "steps = np.arange(0.01, 0.11 , 0.01)\n",
    "for i in range(0, 10):\n",
    "    augment = Compose([AddGaussianNoise(min_amplitude=steps[i], max_amplitude=steps[i], p=1)])\n",
    "    y_dim_5 = augment(samples=y_eval, sample_rate=sr_eval)\n",
    "    track = f'gauss_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_5, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_5, sr_eval, filename)\n",
    "    \n",
    "print(\"Extracted dim 5/10\")\n",
    "\n",
    "# Dimension 6 - TanhDistortion\n",
    "steps = np.arange(0.35, 0.85, 0.05)\n",
    "for i in range(0, 10):\n",
    "    augment = Compose([TanhDistortion(min_distortion = steps[i], max_distortion = steps[i], p=1)])\n",
    "    y_dim_6 = augment(samples=y_eval, sample_rate=sr_eval)\n",
    "    track = f'tanh_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_6, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_6, sr_eval, filename)\n",
    "        \n",
    "print(\"Extracted dim 6/10\")\n",
    "\n",
    "# Dimension 7 - bit crush\n",
    "steps = np.arange(7.75, 5.25, -0.25)\n",
    "board = Pedalboard([Bitcrush()])\n",
    "for i in range(0, 10):\n",
    "    board[0].bit_depth = steps[i]\n",
    "    y_dim_7 = board(y_eval, sr_eval)\n",
    "    track = f'bitcrush_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_7, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_7, sr_eval, filename)\n",
    "        \n",
    "print(\"Extracted dim 7/10\")\n",
    "\n",
    "# Dimension 8 - chorus\n",
    "steps = np.arange(1, 11, 1)\n",
    "board = Pedalboard([Chorus()])\n",
    "for i in range(0, 10):\n",
    "    board[0].rate_hz = steps[i]\n",
    "    #board[0].depth = i\n",
    "    y_dim_8 = board(y_eval, sr_eval)\n",
    "    track = f'Chorus_rate_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_8, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_8, sr_eval, filename)\n",
    "    \n",
    "print(\"Extracted dim 8/10\")\n",
    "\n",
    "# Dimension 9 - phaser\n",
    "steps = np.arange(1, 11, 1)\n",
    "board = Pedalboard([Phaser()])\n",
    "for i in range(0, 10):\n",
    "    board[0].rate_hz = steps[i]\n",
    "    board[0].depth = 0.5\n",
    "    y_dim_9 = board(y_eval, sr_eval)\n",
    "    track = f'Phaser_rate_{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_9, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_9, sr_eval, filename)\n",
    "    \n",
    "print(\"Extracted dim 9/10\")\n",
    "\n",
    "# Dimension 10 - Gain\n",
    "steps = np.arange(1, 11, 1)\n",
    "board = Pedalboard([Gain()])\n",
    "for i in range(0, 10):\n",
    "    board[0].gain_db = steps[i]\n",
    "    y_dim_9 = board(y_eval, sr_eval)\n",
    "    track = f'gain{i+1}'\n",
    "    if (feature_set == 0):\n",
    "        extractTrackDataAndWriteToCSV(y_dim_9, sr_eval, filename)\n",
    "    else:\n",
    "        extractTrackDataAndWriteToCSV_2(y_dim_9, sr_eval, filename)\n",
    "\n",
    "print(\"Extracted dim 10/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66bae8-6579-4f7e-90d4-15b1db8cb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(30, 330, 30):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b37ca3-cd76-4ae6-97d8-df4a11965a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "#eval_data = pd.read_csv('eval.csv')\n",
    "eval_data = pd.read_csv('eval_69.csv')\n",
    "eval_data_track_names = eval_data['track_name']\n",
    "eval_data.drop(['track_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded27767-d197-48b2-b906-55908f7cc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the data \n",
    "eval_data[eval_data.columns] = scaler.fit_transform(eval_data[eval_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb509360-05c2-46d2-bc85-ac2870d4668a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "X = eval_data.copy()\n",
    "X_embedded = sklearn.manifold.TSNE(n_components=2, perplexity=10, learning_rate=10, init='pca').fit_transform(X)\n",
    "\n",
    "# Create empty array\n",
    "categories = np.zeros(X.shape[0])\n",
    "\n",
    "groups = np.array(['Original','BPM','Pitch','High Pass Filter','Low Pass Filter','Gaussion Noise','Tanh Distortion','Bit Crush','Chorus','Phaser','Gain'])\n",
    "colormap = np.array(['black', 'darkred', 'green', 'blue', 'yellow', 'cyan', 'purple', 'orangered', 'gold', 'magenta', 'pink'])\n",
    "for i in range(0, X.shape[0]):    \n",
    "    if (i == 0):\n",
    "        categories[i] = 0\n",
    "    elif ( 1 <= i <= 10):\n",
    "        categories[i] = 1\n",
    "    elif ( 11 <= i <= 20):\n",
    "        categories[i] = 2\n",
    "    elif ( 21 <= i <= 30):\n",
    "        categories[i] = 3\n",
    "    elif ( 31 <= i <= 40):\n",
    "        categories[i] = 4\n",
    "    elif ( 41 <= i <= 50):\n",
    "        categories[i] = 5\n",
    "    elif ( 51 <= i <= 60):\n",
    "        categories[i] = 6\n",
    "    elif ( 61 <= i <= 70):\n",
    "        categories[i] = 7\n",
    "    elif ( 71 <= i <= 80):\n",
    "        categories[i] = 8\n",
    "    elif ( 81 <= i <= 90):\n",
    "        categories[i] = 9\n",
    "    elif ( 91 <= i <= 100):\n",
    "        categories[i] = 10\n",
    "\n",
    "# colorbar and annotations \n",
    "plt.scatter(X_embedded[:,0], X_embedded[:,1], s=100, c=colormap[categories.astype(int)])\n",
    "\n",
    "# Add Legend\n",
    "items = []\n",
    "for i in range(0,groups.size):\n",
    "    items.append(Line2D([0], [0], marker='o', color='whitesmoke', label=groups[i], markerfacecolor=colormap[i], markersize=10))\n",
    "    plt.legend(handles=items)\n",
    "\n",
    "# Add annotations\n",
    "for i in range(0, eval_data_track_names.shape[0]):\n",
    "    if(i==0):\n",
    "        plt.text(X_embedded[i,0]+0.3, X_embedded[i,1]-0.3, eval_data_track_names[i], horizontalalignment='left', size=18, color='black', weight='semibold')\n",
    "    elif(i%5==0): # else:\n",
    "        plt.text(X_embedded[i,0]+0.3, X_embedded[i,1]-0.25, eval_data_track_names[i], horizontalalignment='left', size=13, color='black', weight='semibold')\n",
    "        \n",
    "plt.title('T-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3394cb-e30a-4629-a6ce-762872ec2cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = eval_data.copy()\n",
    "\n",
    "# Perform DR down to 3D \n",
    "X_embedded = sklearn.manifold.TSNE(n_components=3, perplexity=10, learning_rate=10, init='random').fit_transform(X)\n",
    "\n",
    "# creating a list of column names\n",
    "column_values = ['x', 'y', 'z']\n",
    "  \n",
    "# creating the dataframe\n",
    "df = pd.DataFrame(data = X_embedded, columns = column_values)\n",
    "\n",
    "trace_0 = go.Scatter3d(x=(df.x).iloc[0:1], y=(df.y).iloc[0:1], z=(df.z).iloc[0:1], mode='markers', marker=dict(size=15, color=colormap[0], opacity=0.9), name='Original')\n",
    "trace_1 = go.Scatter3d(x=(df.x).iloc[1:11], y=(df.y).iloc[1:11], z=(df.z).iloc[1:11], mode='markers', marker=dict(size=10, color=colormap[1], opacity=0.75), name='BPM')\n",
    "trace_2 = go.Scatter3d(x=(df.x).iloc[11:21], y=(df.y).iloc[11:21], z=(df.z).iloc[11:21], mode='markers', marker=dict(size=10, color=colormap[2], opacity=0.75), name='Pitch')\n",
    "trace_3 = go.Scatter3d(x=(df.x).iloc[21:31], y=(df.y).iloc[21:31], z=(df.z).iloc[21:31], mode='markers', marker=dict(size=10, color=colormap[3], opacity=0.75), name='High Pass Filter')\n",
    "trace_4 = go.Scatter3d(x=(df.x).iloc[31:41], y=(df.y).iloc[31:41], z=(df.z).iloc[31:41], mode='markers', marker=dict(size=10, color=colormap[4], opacity=0.75), name='Low Pass Filter')\n",
    "trace_5 = go.Scatter3d(x=(df.x).iloc[41:51], y=(df.y).iloc[41:51], z=(df.z).iloc[41:51], mode='markers', marker=dict(size=10, color=colormap[5], opacity=0.75), name='Gaussian Noise')\n",
    "trace_6 = go.Scatter3d(x=(df.x).iloc[51:61], y=(df.y).iloc[51:61], z=(df.z).iloc[51:61], mode='markers', marker=dict(size=10, color=colormap[6], opacity=0.75), name='Tanh Distortion')\n",
    "trace_7 = go.Scatter3d(x=(df.x).iloc[61:71], y=(df.y).iloc[61:71], z=(df.z).iloc[61:71], mode='markers', marker=dict(size=10, color=colormap[7], opacity=0.75), name='Bit Crush')\n",
    "trace_8 = go.Scatter3d(x=(df.x).iloc[71:81], y=(df.y).iloc[71:81], z=(df.z).iloc[71:81], mode='markers', marker=dict(size=10, color=colormap[8], opacity=0.75), name='Chorus')\n",
    "trace_9 = go.Scatter3d(x=(df.x).iloc[81:91], y=(df.y).iloc[81:91], z=(df.z).iloc[81:91], mode='markers', marker=dict(size=10, color=colormap[9], opacity=0.75), name='Phaser')\n",
    "trace_10 = go.Scatter3d(x=(df.x).iloc[91:101], y=(df.y).iloc[91:101], z=(df.z).iloc[91:101], mode='markers', marker=dict(size=10, color=colormap[10], opacity=0.75), name='Gain')\n",
    "data_ = [trace_0, trace_1, trace_2, trace_3, trace_4, trace_5, trace_6, trace_7, trace_8, trace_9, trace_10]\n",
    "\n",
    "layout = go.Layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig = go.Figure(data=data_, layout=layout)\n",
    "fig.update_layout(autosize=False, width=1000, height=800,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edb99e-a960-4c88-aa6c-2b79d87e94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pacmap\n",
    "import pacmap\n",
    "\n",
    "X2 =  eval_data.copy()\n",
    "X2_embedded = pacmap.PaCMAP(n_components=2, n_neighbors=3, MN_ratio=3, FP_ratio=4)\n",
    "X2_transformed = X2_embedded.fit_transform(X2.values, init='pca')\n",
    "\n",
    "# Create empty array\n",
    "categories = np.zeros(X.shape[0])\n",
    "\n",
    "groups = np.array(['Original','BPM','Pitch','High Pass Filter','Low Pass Filter','Gaussion Noise','Tanh Distortion','Bit Crush','Chorus','Phaser', 'Gain'])\n",
    "colormap = np.array(['black', 'darkred', 'green', 'blue', 'yellow', 'cyan', 'purple', 'orangered', 'gold', 'magenta', 'pink'])\n",
    "for i in range(0, X.shape[0]):    \n",
    "    if (i == 0):\n",
    "        categories[i] = 0\n",
    "    elif ( 1 <= i <= 10):\n",
    "        categories[i] = 1\n",
    "    elif ( 11 <= i <= 20):\n",
    "        categories[i] = 2\n",
    "    elif ( 21 <= i <= 30):\n",
    "        categories[i] = 3\n",
    "    elif ( 31 <= i <= 40):\n",
    "        categories[i] = 4\n",
    "    elif ( 41 <= i <= 50):\n",
    "        categories[i] = 5\n",
    "    elif ( 51 <= i <= 60):\n",
    "        categories[i] = 6\n",
    "    elif ( 61 <= i <= 70):\n",
    "        categories[i] = 7\n",
    "    elif ( 71 <= i <= 80):\n",
    "        categories[i] = 8\n",
    "    elif ( 81 <= i <= 90):\n",
    "        categories[i] = 9\n",
    "    elif ( 91 <= i <= 100):\n",
    "        categories[i] = 10\n",
    "\n",
    "# colorbar and annotations \n",
    "plt.scatter(X2_transformed[:,0], X2_transformed[:,1], s=100, c=colormap[categories.astype(int)])\n",
    "\n",
    "# Add Legend\n",
    "items = []\n",
    "for i in range(0, groups.size):\n",
    "    items.append(Line2D([0], [0], marker='o', color='whitesmoke', label=groups[i], markerfacecolor=colormap[i], markersize=10))\n",
    "    plt.legend(handles=items)\n",
    "\n",
    "# Add annotations\n",
    "for i in range(0, eval_data_track_names.shape[0]):\n",
    "    if(i==0):\n",
    "        plt.text(X2_transformed[i,0]+0.1, X2_transformed[i,1]-0.2, eval_data_track_names[i], horizontalalignment='left', size=18, color='black', weight='semibold')\n",
    "    elif(i%5==0): # else:\n",
    "        plt.text(X2_transformed[i,0]+0.1, X2_transformed[i,1]-0.1, eval_data_track_names[i], horizontalalignment='left', size=13, color='black', weight='semibold')\n",
    "        \n",
    "plt.title('PaCMAP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a0386-b55c-4102-a8bc-f2dcc3f7033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = eval_data.copy()\n",
    "\n",
    "# Perform DR down to 3D\n",
    "X2_embedded = pacmap.PaCMAP(n_components=3, n_neighbors=3, MN_ratio=3, FP_ratio=4)\n",
    "X2_transformed = X2_embedded.fit_transform(X2.values, init='pca')\n",
    "\n",
    "# creating a list of column names\n",
    "column_values = ['x', 'y', 'z']\n",
    "  \n",
    "# creating the dataframe\n",
    "df = pd.DataFrame(data = X2_transformed, columns = column_values)\n",
    "\n",
    "trace_0 = go.Scatter3d(x=(df.x).iloc[0:1], y=(df.y).iloc[0:1], z=(df.z).iloc[0:1], mode='markers', marker=dict(size=15, color=colormap[0], opacity=0.9), name='Original')\n",
    "trace_1 = go.Scatter3d(x=(df.x).iloc[1:11], y=(df.y).iloc[1:11], z=(df.z).iloc[1:11], mode='markers', marker=dict(size=10, color=colormap[1], opacity=0.75), name='BPM')\n",
    "trace_2 = go.Scatter3d(x=(df.x).iloc[11:21], y=(df.y).iloc[11:21], z=(df.z).iloc[11:21], mode='markers', marker=dict(size=10, color=colormap[2], opacity=0.75), name='Pitch')\n",
    "trace_3 = go.Scatter3d(x=(df.x).iloc[21:31], y=(df.y).iloc[21:31], z=(df.z).iloc[21:31], mode='markers', marker=dict(size=10, color=colormap[3], opacity=0.75), name='High Pass Filter')\n",
    "trace_4 = go.Scatter3d(x=(df.x).iloc[31:41], y=(df.y).iloc[31:41], z=(df.z).iloc[31:41], mode='markers', marker=dict(size=10, color=colormap[4], opacity=0.75), name='Low Pass Filter')\n",
    "trace_5 = go.Scatter3d(x=(df.x).iloc[41:51], y=(df.y).iloc[41:51], z=(df.z).iloc[41:51], mode='markers', marker=dict(size=10, color=colormap[5], opacity=0.75), name='Gaussian Noise')\n",
    "trace_6 = go.Scatter3d(x=(df.x).iloc[51:61], y=(df.y).iloc[51:61], z=(df.z).iloc[51:61], mode='markers', marker=dict(size=10, color=colormap[6], opacity=0.75), name='Tanh Distortion')\n",
    "trace_7 = go.Scatter3d(x=(df.x).iloc[61:71], y=(df.y).iloc[61:71], z=(df.z).iloc[61:71], mode='markers', marker=dict(size=10, color=colormap[7], opacity=0.75), name='Bit Crush')\n",
    "trace_8 = go.Scatter3d(x=(df.x).iloc[71:81], y=(df.y).iloc[71:81], z=(df.z).iloc[71:81], mode='markers', marker=dict(size=10, color=colormap[8], opacity=0.75), name='Chorus')\n",
    "trace_9 = go.Scatter3d(x=(df.x).iloc[81:91], y=(df.y).iloc[81:91], z=(df.z).iloc[81:91], mode='markers', marker=dict(size=10, color=colormap[9], opacity=0.75), name='Phaser')\n",
    "trace_10 = go.Scatter3d(x=(df.x).iloc[91:101], y=(df.y).iloc[91:101], z=(df.z).iloc[91:101], mode='markers', marker=dict(size=10, color=colormap[10], opacity=0.75), name='Gain')\n",
    "data_ = [trace_0, trace_1, trace_2, trace_3, trace_4, trace_5, trace_6, trace_7, trace_8, trace_9, trace_10]\n",
    "\n",
    "layout = go.Layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig = go.Figure(data=data_, layout=layout)\n",
    "fig.update_layout(autosize=False, width=1000, height=800,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64abde-6321-4ee9-8fb0-7b8dfa713703",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extracting 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42f2ee-b8c7-4f7b-996f-ac85bef4a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "root_folder = \"track_library\"\n",
    "lst_2 = list(set(os.listdir(f'{root_folder}')) - {'desktop.ini', '.ipynb_checkpoints'})\n",
    "\n",
    "for i in range(0, len(startingpoints)):\n",
    "    # Write out audio as 24bit PCM WAV\n",
    "    track = lst_2[i]\n",
    "    #print (i, track)  \n",
    "    y, sr = librosa.load(f'track_library/{track}', mono=True, sr=None, offset=startingpoints[i], duration=30)\n",
    "    #sf.write(f'tracks_30s/{track}', y, sr, subtype='PCM_24')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
